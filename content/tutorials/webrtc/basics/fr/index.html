{% extends "tutorial.html" %}

{% block pagebreadcrumb %}{{ tut.title }}{% endblock %}

{% block html5badge %}
<img src="/static/images/identity/html5-badge-h-multimedia.png" width="133" height="64" alt="This article is powered by HTML5 Audio/Video" title="This article is powered by HTML5 Audio?/Video" />
{% endblock %}

{% block iscompatible %}
return !! (window.RTCPeerConnection || window.webkitDeprecatedPeerConnection || window.webkitRTCPeerConnection);
{% endblock %}

{% block translator %}
<div class="translator">
    <strong>Translator:</strong> <a href="https://github.com/gasp">Gaspard Beernaert</a>
</div>
{% endblock %}

{% block head %}
<style>
.talkinghead:before {
  background-image: url(/static/images/profiles/75/dutton.75.png);
}
</style>
{% endblock %}

{% block content %}

<blockquote>
  WebRTC est le nouveau front dans la longue guerre pour un web ouvert et décongestionné.
  <cite><a href="http://hacks.mozilla.org/2012/03/video-mobile-and-the-open-web/" title="Brendan Eich blog post: Video, Mobile, and the Open Web">Brendan Eich</a>, inventor of JavaScript</cite>
</blockquote>

<h2 id="toc-disruptive">Communication en temps réel sans plugins</h2>

<p>Imaginez un monde où votre téléphone, votre télé et votre ordinateur pourraient tous communiquer sur une plateforme commune. Imaginez du chat vidéo et de l'échange de données en peer-to-peer ajoutés en toute facilement à votre application web. Telle est la vision de WebRTC.</p>

<p>Vous voulez essayer ? WebRTC est disponible dès maintenant dans Google Chrome, Opera et Firefox. Une bonne manière de commencer est de lancer cette simple application de chat vidéo sur <a href="http://apprtc.appspot.com" title="Simple WebRTC demo" target="_blank">apprtc.appspot.com</a>:</p>

<ol>
	<li>Ouvrez <a href="http://apprtc.appspot.com" title="Simple WebRTC demo" target="_blank">apprtc.appspot.com</a> dans Chrome, Opera ou Firefox.</li>
	<li>Cliquez sur le bouton "accepter" pour laisser l'applicatiion utiliser votre webcam.</li>
	<li>Ouvre l'URL affichée en bas de la page dans on nouvel onglet, ou, encore mieux, sur un autre ordinateur.</li>
</ol>

 <p>Il y a une version du code commenté de cette application <a href="#toc-simple" title="Code walkthrough of apprtc.appspot.com">un peu plus loin dans cet article</a>.</p>

<h2 id="toc-tldr">Prise en main</h2>

<p>Vous n'avez pas le temps de lire cet article ou vous voulez juste le code?</p>

<ol>
  <li>
    <p>Suivez une présentation de WebRTC lors de la conférence Google I/O (les supports sont <a href="http://io13webrtc.appspot.com" title="Google I/O 2013 WebRTC presentation">là</a>):</p>
    <iframe width="560" height="315" src="http://www.youtube.com/embed/p2HzZkd2A40" frameborder="0" allowfullscreen></iframe>
  </li>
  <li>Si vous n'avez jamais utilisé getUserMedia,jetez un œil à <a href="http://www.html5rocks.com/en/tutorials/getusermedia/intro/" title="HMTL5 Rocks: Capturing Audio and Video in HTML5" target="_blank">l'article sur HTML5 Rocks</a> sur le sujet et parcourrez la source d'un exemple simplifié sur <a href="http://www.simpl.info/getusermedia" title="Simple getUserMedia example">simpl.info/gum</a>.</li>
  <li>Attaquez-vous à l'API de RTCPeerConnection API en parcourant le <a href="#simpleRTCPeerConnectionExample" title="Internal link to simple RTCPeerConnecton example">petit exemple ci-dessous</a> et la démo sur <a href="http://www.simpl.info/pc" title="WebRTC demo without signaling">simpl.info/pc</a>, qui implémente WebRTC sur une seule page web.</li>
	<li>Approfondissez vos connaissances sur la manière dont WebRTC utilise des serveurs pour la signalisation, le passage des firewalls et des NAT en lisant le code et les console logs de <a href="http://apprtc.appspot.com" title="Simple WebRTC video chat demo">apprtc.appspot.com</a>.</li>
</ol>

<p style="font-weight: bold">À défaut, plongez directement dans notre <a href="https://www.bitbucket.org/webrtc/codelab" title="WebRTC codelab repository on Bitbucket">codelab WebRTC</a> : un guide pas à pas expliquant comment construire une application de chat vidéo en utilisant un simple serveur de signalisation.</p>

<h2 id="toc-history">La petite histoire de WebRTC</h2>

<p>Un des derniers grands défis pour le web est de permettre la communication des personnes au par la voix et la vidéo : la communication en temps réel, ou Real Time Communication abrégée RTC. Dans une application web, la communication en temps réel devrait être aussi naturelle que de saisir des mots dans un champ texte. Sans elle, nous sommes limités dans notre capacité à innover et développer de nouveaux médiums d'interaction pour les gens</p>

<p>Historiquement, faire de la communication en temps réel était complexe et nécessitait des technologies propriétaires et coûteuses ou développées en silos. Du coup, intégrer des technologies RTC avec du contenu préexistant c'était assez galère, spécialement sur le web.</p>


<p>Le chat vidéo de Gmail est devenu populaire en 2008 et en 2011 Google a introduit Hangouts qui utilisait le service Google Talk, comme Gmail. Google a racheté GIPS, une entreprise qui avait développé de nombreux composants logiciels requis pour la création de RTC comme des codecs et des dispositifs d'annulation d'écho. Google a rendues open source les technologies développées par GIPS et s'est engagé dans la standardisation d'une norme à l'IETF et au W3C pour s'assurer d'un consensus. En mai 2011, Ericsson a développé <a href="http://www.ericsson.com/research-blog/context-aware-communication/beyond-html5-peer-peer-conversational-video/" title="Beyond HTML5: peer to peer conversational video">la première implémentation de WebRTC</a>.</p>

<p>WebRTC a maintenant des standards ouverts et implémentés pour le des dispositifs de communication audio, vidéo et data en temps réel et sans plugin. Il y a un vrai besoin:</p>

  <ul>
    <li>De nombreux web services utilisend déjà ce genre de technologie mais nécessitent de télécharger une appli ou un plugin. C'est le cas de Skype, Facebook (qui utlise Skype) et Google Hangouts (qui utilise le plugin Google Talk).</li>
    <li>Télécharger, installer et mettre à jour ses plugins peut être compliqué, générateur d'erreurs et ennyeux.</li>
    <li>Les plugins pevent être difficile à déployer, débuger, corriger, tester et maintenir&mdash;et nécessitent souvent un jeu de licenses compliqué et une intégration avec un jeu de technologies complexes et coûteuses. Et surtout, c'est bien difficile de convaincre les utilisateurs d'installer un plugin sur leur machine !</li>
  </ul>

<p>Le fil rouge du projet WebRTC est que ses APIs doivent être open-source, libres, standardisées, inclues dans les navigateur et plus efficaces que l'état de l'art.</p>

<h2 id="toc-where">Où en sommes-nous maintenant ?</h2>

<p>WebRTC implémente trois APIs:</p>
<ul>
  <li><a href="#toc-mediastream" title="Internal link to section for MediaStream (aka getUserMedia)"><code>MediaStream</code></a> (aka <code>getUserMedia</code>)</li>
  <li><code><a href="#toc-rtcpeerconnection" title="Internal link to section for RTCPeerConnection">RTCPeerConnection</a></code></li>
  <li><a href="#toc-rtcdatachannel" title="Internal link to section about RTCDataChannel"><code>RTCDataChannel</code></a></li>
</ul>

<p><code><strong>getUserMedia</strong></code> est disponible dans Chrome, Opera et Firefox. Allez jeter-un œil sur les démos cross-browser sur <a href="http://www.simpl.info/gum" title="Simple cross-platform getUserMedia demo">simpl.info/gum</a> et <a href="http://webaudiodemos.appspot.com/" title="">les super exemples de Chris Wilson</a> qui utilisent <code>getUserMedia</code> comme source pour du traîtement avec Web Audio.</p>

<p><code><strong>RTCPeerConnection</strong></code> est disponible dans Chrome (sur ordi et sur Android), Opera (sur ordi et sur les dernières Android Beta) et dans Firefox. Un petit point sur le nom : après plusieurs itérations, <code>RTCPeerConnection</code> est actuellement intégré dans Chrome et Opera en tant que <code>webkitRTCPeerConnection</code> et dans Firefox en tant que <code>mozRTCPeerConnection</code>. D'autres noms et implémentations on été dépréciées. Quand les standards se seront stabilisés, les préfixes seront supprimés. Il y a une démo super-simple de l'implémentation de RTCPeerConnection dans Chromium sur <a href="http://www.simpl.info/peerconnection" title="Simple cross-platform getUserMedia demo">simpl.info/pc</a> et une chouette application vidéo sur <a href="http://apprtc.appspot.com" title="Video chat demo">apprtc.appspot.com</a>. Cette appli utilise <a href="https://apprtc.appspot.com/js/adapter.js" title="adapter.js JavaScript file">adapter.js</a>, un petit fichier JavaScript qui s'occupe de la correspondance entre les différentes terminologies. Maintenu par Google, il permet de s'abstraire des différences entre les navigateurs et le specs.</p>

<p><strong><code>RTCDataChannel</code></strong> est disponible dans Chrome 25, Opera 18 et Firefox 22 et ultérieurs.</p>

<p>Les fonctionnalités de WebRTC sont disponibles dans Internet Explorer <a href="https://groups.google.com/forum/#!topic/discuss-webrtc/tKoh1wrI8ig" title="How to enable WebRTC functionality in Internet Explorer via Chrome Frame">via Chrome Frame</a>, et Skype (racheté par Microsoft en 2011) prévoit <a href="http://gigaom.com/2012/06/26/skype-webrtc-web-client/" title="GigaOM blog post about Skype and WebRTC">d'utiliser WebRTC</a>. WebRTC a aussi été intégré par les applis natives <a href="https://labs.ericsson.com/developer-community/blog/beyond-html5-conversational-voice-and-video-implemented-webkit-gtk" title="Ericsson article about WebKitGTK+">WebKitGTK+</a> et <a href="http://www.youtube.com/watch?v=Vm5ebKWKNE8" title="basysKom showcasing WebRTC based video chat in QML application">Qt</a>.</p>

<h3>Avertissement</h3>

<p>Certains rapports annoncent que telle ou telle plateforme 'supporte WebRTC' et doivent être traîtés avec scepticisme. En effet, il y a souvent confusion entre WebRTC et <code>getUserMedia</code>, et en effet la plateforme le supporte mais pas les autres composants de WebRTC.</p>

<h2 id="toc-first">Mon premier WebRTC</h2>

<p>Les applications WebRTC nécessitent de faire plusieurs choses :</p>

<ul>
  <li>Récupérer des flux audio, vidéo ou autres données.</li>
  <li>Récupérer les informations du réseau comme les adresses IP et les ports et l'éc, and exchange this with other WebRTC clients (known as <em>peers</em>) to enable connection, even through <a href="http://en.wikipedia.org/wiki/NAT_traversal" title="Wikipedia article: Network Address Translation traversal">NATs</a> and firewalls.</li>
  <li>Coordinate signaling communication to report errors and initiate or close sessions.</li>
  <li>Exchange information about media and client capability, such as resolution and codecs.</li>
	<li>Communicate streaming audio, video or data.</li>
</ul>

<p>Pour récupérer et communiquer des flux de données, WebRTC implémente les APIs suivantes :</p>
<ul>
	<li><a href="https://dvcs.w3.org/hg/audio/raw-file/tip/streams/StreamProcessing.html" title="MediaStream API documentation">MediaStream</a> : avoir accès aux flux de données comme la caméra et le micro de l'utilisateur.</li>
	<li><a href="http://dev.w3.org/2011/webrtc/editor/webrtc.html#rtcpeerconnection-interface" title="W3CRTCPeerConnection Editor's draft">RTCPeerConnection</a>: appels audio ou vidéo avec quelques utilitaires pour l'encryption et la gestion de la bande passante.</li>
	<li><a href="http://dev.w3.org/2011/webrtc/editor/webrtc.html#rtcdatachannel" title="W3C WebRTC RTCDataChannel Editor's draft">RTCDataChannel</a>: échange en peer-to-peer de données.</li>
</ul>

<p>(Vous trouverez les détails de la gestion des aspects signalisation et réseau de WebRTC <a href="#signaling" title="Internal link to section about signaling">ci-dessous</a>.)</p>

<h2 id="toc-mediastream">MediaStream (ou getUserMedia)</h2>

<p>L'<a href="http://dev.w3.org/2011/webrtc/editor/getusermedia.html" title="W3C Editor's Draft: Media Capture and Streams">API MediaStream </a> gère la synchronisation de flux de données. Par exemple, parmi les flux venant de la caméra et du micro, les pistes d'image et de son sont synchronisés. (Ne mélangez pas les pistes de MediaStream (tracks) en anglais avec l'élément &lt;track&gt; qui est <a href="http://www.html5rocks.com/en/tutorials/track/basics/" title="HTML5 Rocks: Getting Started With the Track Element">un tout autre sujet</a> en HTML.)</p>

<p>La meilleure manière de comprendre MediaStream est certainement de voir cette sélection démos parmi celles qui fleurissent sur le Web :</p>

<ol>
  <li>Dans Chrome or Opera, ouvrez la démo sur <a href="http://simpl.info/getusermedia/" title="simpl.info getUserMedia demo">simpl.info/gum</a>.</li>
  <li>Ouvrez la console.</li>
  <li>Inspectez la variable <code>stream</code> qui est dans le scope global.</li>
</ol>

<p>Chaque MediaStream a un input (iuune entrée) qui est un MediaStream généré par <code>navigator.getUserMedia()</code> et un output (une sortie) qui est envoyé vers un élément <code>video</code> ou une connexion RTCPeerConnection.</p>

<p>La méthode <code>getUserMedia()</code> prend 3 arguments :</p>

<ul>
  <li>Un <a href="#toc-constraints" title="Internal link to section about Media Constraints">objet contrainte</a>.</li>
  <li>Un callback en cas de succès qui renvoie un MediaStream.</li>
  <li>Un callback en cas d'échec qui renvoie un objet de type error.</li>
</ul>

<p>Chaque MediaStream a un <code>label</code>, du grenre 'Xk7EuLhsuHKbnjLWkW4yYGNJJ8ONsgwHBvLQ'. Un tableau de MediaStreamTracks est renvoyé par les méthode <code>getAudioTracks()</code> et <code>getVideoTracks()</code>.</p>

<p>Dans l'exempe <a href="http://simpl.info/getusermedia/" title="simpl.info getUserMedia demo">simpl.info/gum</a>, <code>stream.getAudioTracks()</code> renvoie un tableau vide (parce qu'il n'y a pas de son) et, si vous disposez d'une webcam qui fonctionne, <code>stream.getVideoTracks()</code> renvoie un tableau contenant un MediaStreamTrack pour le flux vidéo. Chaque MediaStreamTrack a un attribut <code>kind</code> qui renvoie 'audio' ou 'video' et un <code>label</code> qui renvoie quelque chose du genre 'FaceTime HD Camera' et représente un ou plusieurs canaux audio ou vidéo. Dans ce cas, on n'a qu'une piste vidéo sans piste audio, mais on pourrait imaginer une application de chat qui récupèrerait les flux des deux caméras de chaque côté d'un téléphone mobile, du micro et d'une application de partage d'écran.</p>

<p>Dans Chrome ou Opera, la méthode <code>URL.createObjectURL()</code> convertit un MediaStream en <a href="http://www.html5rocks.com/tutorials/workers/basics/#toc-inlineworkers-bloburis" title="HTML5 Rocks: Blob URLs">Blob URL</a> quipeut être défini comme <code>src</code> d'un élément videoelement. (Dans Firefox et Opera, le <code>src</code> de la vidéo peut appeler le flux lui-même.) Depuis leur version 25, les navigateurs à base de Chromium (Chrome et Opera) autorisent le flux de données audio à être péssé depuis <code>getUserMedia</code> vers un élément audio ou video (mais prenez en compte que par défaut, l'élément média ne sera pas en sourdine).<p>

<p><code>getUserMedia</code> peut aussi être utilisé <a href="http://updates.html5rocks.com/2012/09/Live-Web-Audio-Input-Enabled" title="HTML5 Rocks update from Chris Wilson: live Web Audio input enabled!">comme source pour l'API Web Audio</a> :</p>

<pre class="prettyprint">
function gotStream(stream) {
    window.AudioContext = window.AudioContext || window.webkitAudioContext;
    var audioContext = new AudioContext();

    // Crée un AudioNode à partir du flux
    var mediaStreamSource = audioContext.createMediaStreamSource(stream);

    // Connectez le à une destination pour vous entendre
    // ou à n'importe quoi d'autre pour effectuer du traîtement!
    mediaStreamSource.connect(audioContext.destination);
}

navigator.getUserMedia({audio:true}, gotStream);
</pre>

<p>Les applications basées sur Chromium et leurs extepeuvent aussi utiliser <code>getUserMedia</code>. Pour <a href="https://developer.chrome.com/extensions/manifest.html#permissions" title="Chrome app/extension permissions documentation">autoriser</a> <code>audioCapture</code>et/ou <code>videoCapture</code>, ajoutez au manifest et la permission ne sera demandée qu'une seule fois à l'utilisateur lors de l'installation. Dès lors, l'accès à la caméra et au micro seront automatiques.</p>

<p>Dans le même esprit pour les pages qui utilisent HTTPS : la permission n'est demandée qu'une seule fois pour <code>getUserMedia()</code> (dans Chrome du moins). La première fois, un bouton 'Toujous autoriser' s'affiche dans l'<a href="http://dev.chromium.org/user-experience/infobars" title="Chromium information about the browser infobar">infobar</a> du navigateur.</p>

<p>À terme, MediaStream pourrait capturer des flux de n'importe quelle source de données, pas seulement ceux de la caméra et du micro. On pourrait par exemple capter un flux venant du disque ou d'une toute autre source de données comme des capteurs.</p>

<p>Notez que <code>getUserMedia()</code> soit être utilisé sur un serveur, pas directement à partir du système de fichiers sinon une erreur de type <code>PERMISSION_DENIED: 1</code> surviendra.</p>

<p><code>getUserMedia()</code> devient vraiment cool quand on le combine avec d'autres API JavaScrpt :</p>

<ul>
   <li><a href="http://webcamtoy.com/app/" title="Webcam Toy site">Webcam Toy</a> est une application de type photomaton qui utilise WebGL pour ajouter des effets aux photos qu'on peut partager ou enregistrer sur son ordi.</li>
   <li><a href="http://www.shinydemos.com/facekat/" title="FaceKat game">FaceKat</a> est un détecteur de visage qui utilise <a href="headtrackr library for realtime face and head tracking" title="headtrackr.js ">headtrackr.js</a> comme joystick.</li>
   <li><a href="http://idevelop.ro/ascii-camera/" title="'ASCII camera' demo">ASCII Camera</a> utilise l'API de Canvas pour générer des images ASCII images.</li>
</ul>

<figure>
  <a href="http://idevelop.ro/ascii-camera/" title="ASCII Camera app"><img src="ascii.png" alt="ASCII image generated by idevelop.ro/ascii-camera" /></a>
  <figcaption>De l'ASCII art avec GetUserMedia !</figcaption>
</figure>

<h3 id="toc-constraints">Jeux de constraintes</h3>

<p>Les <a href="http://tools.ietf.org/html/draft-alvestrand-constraints-resolution-00#page-4" title="IETF Resolution Constraints draft specification">jeux de constraintes</a> ont été implémentés dans Chrome depuis la version 24 et Opera 18. Elles définissent une liste de valeurs pour les appels à <code>getUserMedia()</code> et <code>addStream()</code> de RTCPeerConnection. Le but est d'implémenter <a href="http://dev.w3.org/2011/webrtc/editor/getusermedia.html#the-model-sources-sinks-constraints-and-states" title="W3C getUserMedia Editor's Draft - The model: sources, sinks, constraints, and states">un support pour d'autres jeux de contraintes</a> comme le ratio d'image, le 'facing mode' (caméra avant ou caméra arrière), frame rate (FPS), hauteur et largeur, dans une unique méthode <a href="http://dev.w3.org/2011/webrtc/editor/getusermedia.html#methods-1" title="applyConstraints() API proposal in W3C getUserMedia Editor's Draft"><code>applyConstraints()</code></a>. </p>

<p>Voir le petit exemple sur <a href="http://simpl.info/getusermedia/constraints/index.html" title="Resolution Constraints example on simpl.info">simpl.info/getusermedia/constraints</a>.</p>

<p>Il y a un hic : le jeu de contraines de <code>getUserMedia</code> apliqués dans une tab d'un navigateur impacte toutes les autres tabs. Attribuer un jeu de contrainte non autorisée renvoie un message d'erreur assez incompréhensible :</p>

<pre class="prettyprint">navigator.getUserMedia error:
NavigatorUserMediaError {code: 1, PERMISSION_DENIED: 1}</pre>

<h4>Capture d'écran et de tab</h4>

<p>Il est aussi possible d'utiliser la capture d'écran comme source de MediaStream. C'est actuellment possible dans Chrome en utilisant le jeu de contraintes experimental chromeMediaSource, comme dans <a href="https://html5-demos.appspot.com/static/getusermedia/screenshare.html" title="Screenshare demo">cette demo</a>. Cette fonctionnalité n'est pas encore dispobible dans Opera. Notez que la capture d'écran nécessite HTTPS.</p>

<p>Les applications basées sur Chrome permettent aussi de partager un flux vidéo dune tab du navigateur via l'API encore expérimentale <a href="http://developer.chrome.com/dev/extensions/tabCapture.html" title="chrome.tabCapture API documentation">chrome.tabCapture</a>. Pour faire du partage d'écran, veuillez consulter le code et les infos connexes sur cet article de HTML5 Rocks : <a href="http://updates.html5rocks.com/2012/12/Screensharing-with-WebRTC" title="HTML5 Rocks update article: Screensharing with WebRTC">Screensharing with WebRTC</a>. Cette fonctionnalité n'esst pas encore disponible sur Opera.</p>

<h2 id="toc-signaling">La signalisation : gestion de session, informations sur réseaux et médias</h2>

<p>WebRTC utilise RTCPeerConnection pour communiquer des flux de données entre les navigateurs (pairs). Il a aussi besoin d'un mécanisme pour coordoner la communication et envoyer des messages de contrôle, c'est ce qu'on appelle la signalisation. Les méthodes de signalisation et les protocols <em>ne sont pas spécifiés</em> par WebRTC : la signalisation n'est pas inclue dans l'API de RTCPeerConnection.</p>

<p>Les développeurs d'applications utilisant WebRTC peuvent donc utiliser la signalisation qu'ils veulent comme SIP ou XMPP, et n'importe quel canal de communication duplex (dans les deux sens). L'exemple <a href="http://apprtc.appspot.com" title="apprtc WebRTC example">apprtc.appspot.com</a> utilise XHR et l'API Channel comme système de signalisation. Dans le <a href="http://www.bitbucket.org/webrtc/codelab" title="WebRTC codelab">codelab</a>, on utilise une instance de <a href="http://Socket.io" title="Socket.io website">Socket.io</a> qui tourne sur un serveur <a href="http://nodejs.org/" title="Node website">Nodejs</a>.</p>

<p>La signalisation est utilisée pour échanger trois types d'info.</p>
<ul>
  <li>Messages de contrôle de session : pour initialiser ou fermer un lien de communication et rapporter les erreurs.</li>
  <li>Configuration réseau : quels sont est l'adresse IP et le port de l'ordinateur ?</li>
  <li>Compatibilité des médias: quels codecs sont gérés et jusqu'à quelle résolution est-ce que mon navigateur et celui de mon interlocuteur vont pouvoir monter ?</li>
</ul>

<p>Ces échanges d'informations par la méthode de signalisation doivent avoir été échangées avec succès avant que le flux de pair à pair ne puisse commencer.</p>

<p>Par exemple, imaginez qu'Alice veuille communiquer avec Bob. Voici un exemple de code provenant du <a href="http://www.w3.org/TR/webrtc/#simple-example" title="WebRTC 1.0: Real-time Communication Between Browsers"> document de travail sur WebRTC auprès du W3C</a> qui montre comment se passe le signaling. Le code assume qu'un système de signaling existe, créé par la méthode <code>createSignalingChannel()</code>. On notera aussi que pour Chrome et Opera, RTCPeerConnection est actuellement préfixé.</p>

<a id="simpleRTCPeerConnectionExample"></a>

<pre class="prettyprint">var signalingChannel = createSignalingChannel();
var pc;
var configuration = ...;

// start(true) pour commencer l'appel
function start(isCaller) {
    pc = new RTCPeerConnection(configuration);

    // envoie un ice candidate à l'autre pair
    pc.onicecandidate = function (evt) {
        signalingChannel.send(JSON.stringify({ "candidate": evt.candidate }));
    };

    // affiche le flux vidéo
    pc.onaddstream = function (evt) {
        remoteView.src = URL.createObjectURL(evt.stream);
    };

    // récupère le flux local, le montre dans la vidéo de prévisu et l'envoie
    navigator.getUserMedia({ "audio": true, "video": true }, function (stream) {
        selfView.src = URL.createObjectURL(stream);
        pc.addStream(stream);

        if (isCaller)
            pc.createOffer(gotDescription);
        else
            pc.createAnswer(pc.remoteDescription, gotDescription);

        function gotDescription(desc) {
            pc.setLocalDescription(desc);
            signalingChannel.send(JSON.stringify({ "sdp": desc }));
        }
    });
}

signalingChannel.onmessage = function (evt) {
    if (!pc)
        start(false);

    var signal = JSON.parse(evt.data);
    if (signal.sdp)
        pc.setRemoteDescription(new RTCSessionDescription(signal.sdp));
    else
        pc.addIceCandidate(new RTCIceCandidate(signal.candidate));
};
</pre>

<p>Pour commencer, Alice et Bob échangent des informations sur la confuguration de leurs réseaux respectifs. (L'expréssion 'recherche de candidats' [ou finding candidates en anglais] fait référence à la recherche d'interface réseaux et de ports en utilisant <a href="#ice" title="Internal link to more information about the ICE framework">le framework ICE</a>.)</p>

<ol>
  <li>Alice crée un objet RTCPeerConnection avec un gestionnaire (handler) <code>onicecandidate</code>.</li>
  <li>Le handler est lancé quand les candidats dont disponibles.</li>
  <li>Alice envoie ses candidats à Bob via la méthode de signaling qu'ils utilisent : WebSocket ou autre mécanisme.</li>
  <li>Quand Bob reçoit les candidats d'Alice, il appelle la méthode <code>addIceCandidate</code> pour les ajouter à sa description de pair distant.</li>
</ol>

<p>les clients WebRTC (connus comme <strong>pairs</strong>, dans notre ca Alice et Bob) ont aussi besoin de vérifier et échanger les informations sur les médias locaux et distants comme la résolution et les codecs. La signalisation de ces configurations s'effectue par l'échange d'une <em>offre</em> et d'une <em>réponse</em> en utilisant le protocole de description de session (Session Description Protocol, SDP):</p>

<ol>
  <li>Alice lance la méthode <code>createOffer()</code> de RTCPeerConnection.
  En callback, elle reçoit un argument de type RTCSessionDescription qui contient la description de la session d'Alice.</li>
  <li>Dans le callback, Alice spécifie la description locale en utilisant <code>setLocalDescription()</code> et dès lors envoie la description de cette session à Bob au travers de leur canal de signalisation. Notez que RTCPeerConnection ne va pas commencer à rechercher de candidats tant que <code>setLocalDescription()</code> est appelé: ceci est spécifié dans <a href="http://tools.ietf.org/html/draft-ietf-rtcweb-jsep-03#section-4.2.4" title="Javascript Session Establishment Protocol draft-ietf-rtcweb-jsep-03">la proposition sur l'JSEP formulée auprès de l'IETF</a>.</li>
  <li>Bob enregistre la description envoyée par alice Alice comme une description distante en utilisant <code>setRemoteDescription()</code>.</li>
  <li>Bob lance la méthode <code>createAnswer()</code> de RTCPeerConnection en lui passant la description distante qu'il a reçu d'Alice afin de génerer une session locale qui lui soit compatible. Le callback de <code>createAnswer()</code> lui renvoie une RTCSessionDescription: Bob l'enregistre comme sa description locale de session et l'envoie à Alice.</li>
  <li>Quand Alice reçoit la description de session de Bob, elle l'enregistre comme la description de la session distante avec <code>setRemoteDescription</code>.</li>
  <li>Hop!</li>
</ol>

<p>Les objets RTCSessionDescription se conforment à la <a href="http://en.wikipedia.org/wiki/Session_Description_Protocol" title="Wikipedia article about the Session Description Protocol">description de protocoles de session (Session Description Protocol)</a>, SDP. Sérialisé, un objet SDP ressemble à ça :</p>

<pre class="prettyprint">
v=0
o=- 3883943731 1 IN IP4 127.0.0.1
s=
t=0 0
a=group:BUNDLE audio video
m=audio 1 RTP/SAVPF 103 104 0 8 106 105 13 126

// ...

a=ssrc:2223794119 label:H4fjnMzxy3dPIgQ7HxuCTLb4wLLLeRHnFxh810
</pre>

<p>L'acquisition et l'échange des informations de médias et de réseaux peut être effectué de manière simultanée, mais doivent être achevés avant que les pairs puissent commencer à échanger leurs flux</p>

<p>L'architecture offre/réponse décrite ci-dessus est appelée <a href="http://tools.ietf.org/html/draft-ietf-rtcweb-jsep-00" title="IETF JSEP draft proposal">JSEP</a> pour JavaScript Session Establishment Protocol.

(Il y a une excellente animation qui explique le procédé de signalisation et d'échange de flux dans <a href="http://www.ericsson.com/research-blog/context-aware-communication/beyond-html5-peer-peer-conversational-video/" title="Ericsson conversational video demo">la vidéo de démo d'Ericsson</a> pour sa première implémentation de WebRTC.) </p>

<figure>
  <img src="jsep.png" alt="JSEP architecture diagram" />
  <figcaption>L'architecture JSEP</figcaption>
</figure>

<p>Une fois que le processus de signalisation s'est correctement déroulé, les données peuvent être transférées directement de pair à pair entre l'appelant et l'appelé &mdash; ou si ça a échoué par l'intermédiaire d'un serveur de relais (on en parle plus bas). .</p>

<p>Once the signaling process has completed successfully, data can be streamed directly peer to peer, between the caller and callee&mdash;or if that fails, via an intermediary relay server (more about that below). Streaming is the job of RTCPeerConnection.</p>

<h2 id="toc-rtcpeerconnection">RTCPeerConnection</h2>

<p>RTCPeerConnection est le composant de WebRTC qui gère l'efficacité et la stabilité des échanges de flux entre les pairs.</p>

<p>Le schéma ci-dessous montre le rôle de RTCPeerConnection. Comme vous pouvez le voir, les zones en vert sont assez complexes !</p>

<figure>
<a href="http://www.webrtc.org/reference/architecture" title="webrtc.org: architecture diagram"><img src="webrtcArchitecture.png" alt="WebRTC architecture diagram" style="width: 740px; height: 482px;" /></a>
<figcaption>architecture de WebRTC (de <a href="http://www.webrtc.org/reference/architecture" title="webrtc.org: architecture diagram">webrtc.org</a>)</figcaption>
</figure>

<p>D'un point de vue de JavaScript, ce qu'il faut comprendre, c'est que RTCPeerConnection protège les développeurs d'une myriade de complexités qui se cachent sous la specification. Les codecs et les protocols utilisé par web RTC fournissent un travail considérable pour rendre possible la communication en temps réel, même sur des réseaux instables :</p>

<ul>
  <li>masquage des effets de la perte de paquets IP</li>
  <li>annulation de l'écho</li>
  <li>adaptation de la bande passante</li>
  <li>jestion du bufferdynamic jitter buffering</li>
  <li>contrôle automatique du gain</li>
  <li>réduction et supprésion du bruit</li>
  <li>'nettoyage' de l'image.</li>
</ul>

<p><a href="#simpleRTCPeerConnectionExample" title="Internal link to W3C RTCPeerConnection example">Le code du W3C ci-dessus</a> montre un exemple simplifié de WebRTC d'un point de vue signal. Vous trouverez ci-dessous un guide pas à pas de deux applications WebRTC: le premier est une simple démonstration de RTCPeerConnection; le second est un client de chat vidéo complet.</p>

<h3 id="toc-sans">RTCPeerConnection sans serveurs</h3>

<p>Le code ci-dessous vient de la démo de WebRTC sur une seule page consultable sur <a href="https://webrtc-demos.appspot.com/html/pc1.html" title="WebRTC demo without signaling">webrtc-demos.appspot.com</a>.Il y a une connection RTCPeerConnection locale <em>et</em> une distante (une vidéo est locale, l'autre distante) sur une seule page. Ça ne sert à rien d'avoir l'appelé et l'appelant sur la même page, mais ça permet de mieux comprendre les objets RTCPeerConnection en s'affranchissant des macanismes de signalisation.</p>

<p>Seul hic: le deuxième jeu de 'contraintes' (optionnel) du constructeur <code>RTCPeerConnection()</code> est différent des jeux de contraintes utilisés lors du <code>getUserMedia()</code>: voir <a href=" http://www.w3.org/TR/webrtc/#constraints" title="W3C Working Draft Constraints section">w3.org/TR/webrtc/#constraints</a> pour plus d'infos.</p>

<p>Dans cet exemple, <code>pc1</code> représente le pair local (l'appelant) et <code>pc2</code> représente le pair distant (appelé).</p>

<h3>L'appelant</h3>

<ol>

<li>
<p>Crée une nouvelle RTCPeerConnection attache le flux de <code>getUserMedia()</code> :</p>
<pre class="prettyprint">
// servers est une config optionnelle (voir la note sur TURN et STUN plus bas)
pc1 = new webkitRTCPeerConnection(servers);
// ...
pc1.addStream(localstream); </pre>
</li>

<li>
<p>Crée une offre et l'associe à la description locale pour <code>pc1</code> et à la description distante pour <code>pc2</code>. Dans ce cas, c'est fait directement dans le code sans utiliser de signalisation puisque l'appelant et l'appelé sont dans la même page.</p>
<pre class="prettyprint">
pc1.createOffer(gotDescription1);
//...
function gotDescription1(desc){
  pc1.setLocalDescription(desc);
  trace("Offer from pc1 \n" + desc.sdp);
  pc2.setRemoteDescription(desc);
  pc2.createAnswer(gotDescription2);
}
</pre>
</li>


</ol>

<h3>L'appelé</h3>

<ol>

<li>
<p>Crée <code>pc2</code> et, quand le flux arrive de <code>pc1</code>, l'affiche dans un élément vidéo : </p>
<pre class="prettyprint">
pc2 = new webkitRTCPeerConnection(servers);
pc2.onaddstream = gotRemoteStream;
//...
function gotRemoteStream(e){
  vid2.src = URL.createObjectURL(e.stream);
}
</pre>
</li>

</ol>

<h3 id="toc-real">RTCPeerConnection avec serveurs</h3>

<p>Dans la vraie vie, WebRTC a besoin de serveurs pour permettre différentes choses :</p>

<ul>
  <li>Les utilisateurs doivent se découvrir et échanger leurs détails respectifs, comme leurs noms.</li>
  <li>les clients WebRTC (pairs) doivent échanger leurs configuraton réseau.</li>
  <li>Les pairs doivent échanger des infos sur leurs capacités comme leurs formats video et resolution.</li>
  <li>Les clients WebRTC doivent passer les barrières des <a href="http://en.wikipedia.org/wiki/NAT_traversal" title="Wikipedia article: Network Address Translation traversal">NAT gateways</a> et des firewalls.</li>
</ul>

<p>En d'autres termes, WebRTC a besoin de quatre types de fonctionnalités fournies par les serveurs :</p>
<ul>
	<li>Découverte d'utilisateurs et communication.</li>
  <li>Signalisation.</li>
	<li>Traversée des NAT/firewall.</li>
  <li>Serveurs de relais au cas ou la communication de pair à pair échoue.</li>
</ul>

<a id="stun"></a>
<a id="ice"></a>


<p>Nous ne traiterons dans cet article ni la traversée des NAT, ni les réseaux de pair à pair, ni les moyens de se faire rencontrer les utilisateurs, ni encore les différents moyens de signalisation, parce qu'on s'écarterait trop de son sujet. Disons pour faire simple que le protocole <a href="http://en.wikipedia.org/wiki/STUN" title="Wikipedia STUN article">STUN</a> et ses extensions <a href="http://en.wikipedia.org/wiki/Traversal_Using_Relay_NAT" title="Wikipedia article about TURN">TURN</a> sont utilisés dans le framework <a href="http://en.wikipedia.org/wiki/Interactive_Connectivity_Establishment" title="Wikipedia article about ICE">ICE</a> pour permettre à RTCPeerConnection de gérer la traversée des NAT et d'autres aléas liés aux réseaux.</p>

<p>ICE est un framework pour connecter les pairs comme deux personnes qui feraient du chat vidéo. Pour commencer, ICE tente de connecter les pairs <em>directement</em>, avec le moins de latence possible, via UDP. Dans ce processus, des serveurs STUN ne font qu'une chose : permettre au pair derrière son NAT de trouver son adresse IP publique et son port. (Google a quelques serveur STUN, l'un d'entre eux est utilisé pour l'exemple sur apprtc.appspot.com.)</p>

<figure>
  <img src="stun.png" alt="Finding connection candidates" />
  <figcaption>Finding connection candidates</figcaption>
</figure>


<p>Si UDP échoue, ICE essaye TCP: d'abord HTTP puis HTTPS. Si la communication en direct échoue &mdash; souvent à cause des NAT ou des firewalls &mdash; ICE utilise un serveur TURN relais. En d'autres mots, ICE va d'abord essayer STUN en UDP pour connecter les pairs entre eux, et si ça échoue, il se rabattra sur un serveur de relais TURN. L'expression 'finding candidates' fait référence au procédé de découverte des interfaces réseaux et des ports.</p>

{% endblock %}
